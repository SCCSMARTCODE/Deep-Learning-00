{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from torchvision.datasets import CIFAR100\nfrom torchvision import transforms as tt\nfrom torch import nn\n# from torchsummary import summary\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.utils.data import DataLoader, random_split\nimport torch\nfrom matplotlib import pyplot as plt\nfrom torch.cuda.amp import autocast, GradScaler\n\n\n# torch.set_default_dtype(torch.float32)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-03T07:13:36.935777Z","iopub.execute_input":"2024-08-03T07:13:36.936177Z","iopub.status.idle":"2024-08-03T07:13:39.590267Z","shell.execute_reply.started":"2024-08-03T07:13:36.936147Z","shell.execute_reply":"2024-08-03T07:13:39.589472Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n\ntransforms = tt.Compose(\n    [\n        tt.ToTensor(),\n        tt.RandomCrop(32, padding=5, padding_mode='reflect'),\n        tt.RandomHorizontalFlip(),\n#         tt.RandomRotation(10),\n        tt.Normalize(*stats, inplace=True)\n    ]\n)\n\n\n\n\ntry:\n    raw_training_dataset = CIFAR100(root='.', train=True, download=False, transform=transforms)\n    raw_testing_dataset = CIFAR100(root='.', train=False, download=False, transform=tt.ToTensor())\n    print(\"Data sync successful\")\nexcept RuntimeError:\n    raw_training_dataset = CIFAR100(root='.', train=True, download=True, transform=transforms)\n    raw_testing_dataset = CIFAR100(root='.', train=False, download=True, transform=tt.ToTensor())\n    print(\"Data sync unsuccessful. Downloaded the dataset.\")\n\n\ntraining_dataset, validation_dataset = random_split(raw_training_dataset, [45000, 5000])\n\ntrain_dl = DataLoader(dataset=training_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\nvalid_dl = DataLoader(dataset=validation_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True, drop_last=True)\ntest_dl = DataLoader(dataset=raw_testing_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True, drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-03T07:13:41.977666Z","iopub.execute_input":"2024-08-03T07:13:41.978601Z","iopub.status.idle":"2024-08-03T07:13:43.182674Z","shell.execute_reply.started":"2024-08-03T07:13:41.978568Z","shell.execute_reply":"2024-08-03T07:13:43.181673Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Data sync successful\n","output_type":"stream"}]},{"cell_type":"code","source":"class DeviceDataLoader:\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n\n    def __iter__(self):\n        for batch_data in self.dl:\n            torch.cuda.empty_cache()\n            yield [t.to(self.device, non_blocking=True) for t in batch_data]\n\n    def __len__(self):\n        return len(self.dl)","metadata":{"execution":{"iopub.status.busy":"2024-08-03T07:13:45.358518Z","iopub.execute_input":"2024-08-03T07:13:45.359072Z","iopub.status.idle":"2024-08-03T07:13:45.365018Z","shell.execute_reply.started":"2024-08-03T07:13:45.359041Z","shell.execute_reply":"2024-08-03T07:13:45.364058Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class CIFAR100Network(nn.Module):\n    def __init__(self):\n        super(CIFAR100Network, self).__init__()\n\n        self.layer0 = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(num_features=32),\n\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(num_features=64),\n\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(num_features=128),\n            nn.MaxPool2d(kernel_size=2)\n        )\n\n        self.resBlock0 = nn.Sequential(\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(num_features=128),\n\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(num_features=128)\n        )\n\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(num_features=256),\n\n            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(num_features=512),\n\n            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(num_features=1024),\n            nn.MaxPool2d(kernel_size=2)\n\n        )\n\n        self.resBlock1 = nn.Sequential(\n            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(num_features=1024),\n\n            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(num_features=1024)\n        )\n\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(in_channels=1024, out_channels=2048, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(num_features=2048),\n\n            nn.Conv2d(in_channels=2048, out_channels=1024, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(num_features=1024),\n            nn.MaxPool2d(kernel_size=2)\n\n        )\n\n        self.resBlock2 = nn.Sequential(\n            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(num_features=1024),\n\n            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(num_features=1024),\n        )\n\n        self.fc_layer = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(in_features=1024 * 4 * 4, out_features=1024),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.3),\n            nn.Linear(in_features=1024, out_features=100)\n        )\n\n    def forward(self, x):\n        x = self.layer0(x)\n        x = x + self.resBlock0(x)\n        x = self.layer1(x)\n        x = x + self.resBlock1(x)\n        x = self.layer2(x)\n        x = x + self.resBlock2(x)\n        x = self.fc_layer(x)\n        return x\n\nmodel = CIFAR100Network()\n\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n\nmodel = model.to(device=device)\n    \n\n# summary(model, (3, 32, 32), batch_size=128)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T07:13:49.688623Z","iopub.execute_input":"2024-08-03T07:13:49.688999Z","iopub.status.idle":"2024-08-03T07:13:50.952588Z","shell.execute_reply.started":"2024-08-03T07:13:49.688972Z","shell.execute_reply":"2024-08-03T07:13:50.951792Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"MAX_LR = 2e-3\nEPOCHS = 50\n\ntrain_dl = DeviceDataLoader(train_dl, device)\nvalid_dl = DeviceDataLoader(valid_dl, device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = Adam(params=model.parameters(), lr=2e-4, weight_decay=.01)\nscheduler = OneCycleLR(optimizer=optimizer, max_lr=MAX_LR, epochs=EPOCHS, steps_per_epoch=len(train_dl))","metadata":{"execution":{"iopub.status.busy":"2024-08-03T07:13:54.646491Z","iopub.execute_input":"2024-08-03T07:13:54.647357Z","iopub.status.idle":"2024-08-03T07:13:54.653379Z","shell.execute_reply.started":"2024-08-03T07:13:54.647323Z","shell.execute_reply":"2024-08-03T07:13:54.652493Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad\ndef evaluate_loss(model, loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    for inputs, targets in loader:\n        inputs, targets = inputs.to(device), targets.to(device)\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        total_loss += loss.item()\n    average_loss = total_loss / len(loader)\n    return average_loss\n\n@torch.no_grad\ndef accuracy(model, loader, device):\n    model.eval()\n    total_correct = 0\n    total_datapoints = 0\n    for inputs, targets in loader:\n        inputs, targets = inputs.to(device), targets.to(device)\n        outputs = model(inputs)\n        preds = torch.argmax(outputs, dim=1)\n        total_correct += (preds == targets).sum().item()\n        total_datapoints += len(targets)\n    total_accuracy = total_correct / total_datapoints\n    return total_accuracy\n\n\n@torch.no_grad\ndef evaluate(model, loader, criterion, device):\n    \"\"\"\n    This function evaluates loss and calculate accuracy same time\n    \"\"\"\n\n    model.eval()\n    total_loss = 0\n    total_correct = 0\n    total_datapoints = 0\n\n    for inputs, targets in loader:\n        inputs, targets = inputs.to(device), targets.to(device)\n        outputs = model(inputs)\n\n        # Calculate loss\n        loss = criterion(outputs, targets)\n        total_loss += loss.item()\n\n        # Calculate accuracy\n        preds = torch.argmax(outputs, dim=1)\n        total_correct += (preds == targets).sum().item()\n        total_datapoints += len(targets)\n        del inputs, targets, outputs, preds\n        torch.cuda.empty_cache()\n\n    average_loss = total_loss / len(loader)\n    total_accuracy = total_correct / total_datapoints\n\n    return average_loss, total_accuracy\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T07:13:57.390996Z","iopub.execute_input":"2024-08-03T07:13:57.391600Z","iopub.status.idle":"2024-08-03T07:13:57.403913Z","shell.execute_reply.started":"2024-08-03T07:13:57.391567Z","shell.execute_reply":"2024-08-03T07:13:57.402576Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"evaluate(model, valid_dl, criterion, device)","metadata":{"execution":{"iopub.status.busy":"2024-08-03T07:14:03.255419Z","iopub.execute_input":"2024-08-03T07:14:03.256297Z","iopub.status.idle":"2024-08-03T07:14:09.203912Z","shell.execute_reply.started":"2024-08-03T07:14:03.256263Z","shell.execute_reply":"2024-08-03T07:14:09.202918Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(4.605046357863989, 0.010216346153846154)"},"metadata":{}}]},{"cell_type":"code","source":"def fit_network(model, epochs, train_dl, valid_dl, criterion, optimizer, scheduler, device):\n    \"\"\"\n    This function trains a neural network and evaluates its performance on a validation set.\n\n    Args:\n        model (nn.Module): The neural network model to train.\n        epochs (int): Number of epochs to train the model.\n        train_dl (DataLoader): DataLoader for the training set.\n        valid_dl (DataLoader): DataLoader for the validation set.\n        criterion (nn.Module): Loss function.\n        optimizer (torch.optim.Optimizer): Optimizer for training the model.\n        scheduler (torch.optim.lr_scheduler): Learning rate scheduler.\n        device (torch.device): Device to run the training on (CPU or GPU).\n\n    Returns:\n        tuple: Training history and validation history.\n    \"\"\"\n\n    train_history = []\n    valid_history = []\n\n    for epoch in range(epochs):\n        model.train()\n        batch_loss = 0\n\n        for inputs, targets in train_dl:\n            inputs, targets = inputs.to(device), targets.to(device)\n            optimizer.zero_grad()\n\n            outputs = model(inputs)\n            train_loss = criterion(outputs, targets)\n            train_loss.backward()\n            optimizer.step()\n\n            batch_loss += train_loss.item()\n            \n            # Free memory\n            del inputs, targets, outputs\n            torch.cuda.empty_cache()\n\n        train_loss = batch_loss / len(train_dl)\n        train_history.append(train_loss)\n\n        # Evaluate on validation set\n        val_loss, val_acc = evaluate(model, valid_dl, criterion, device)\n        valid_history.append((val_loss, val_acc))\n\n        print(f\"{epoch+1}/{epochs}: train_loss[{train_loss:.4f}] - val_loss[{val_loss:.4f}] - val_acc[{val_acc:.4f}]\")\n\n        scheduler.step()\n\n    return train_history, valid_history\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T07:14:18.209983Z","iopub.execute_input":"2024-08-03T07:14:18.210780Z","iopub.status.idle":"2024-08-03T07:14:18.220381Z","shell.execute_reply.started":"2024-08-03T07:14:18.210742Z","shell.execute_reply":"2024-08-03T07:14:18.219311Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def plot_training_history(train_history, valid_history):\n    epochs = len(train_history)\n    train_losses = train_history\n    val_losses = [x[0] for x in valid_history]\n    val_accuracies = [x[1] for x in valid_history]\n\n    plt.figure(figsize=(18, 6))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(range(epochs), train_losses, label='Training Loss')\n    plt.plot(range(epochs), val_losses, label='Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss')\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(range(epochs), val_accuracies, label='Validation Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.title('Validation Accuracy')\n    plt.legend()\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-03T07:14:27.875633Z","iopub.execute_input":"2024-08-03T07:14:27.876023Z","iopub.status.idle":"2024-08-03T07:14:27.884256Z","shell.execute_reply.started":"2024-08-03T07:14:27.875988Z","shell.execute_reply":"2024-08-03T07:14:27.883275Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_history, valid_history = fit_network(model, EPOCHS, train_dl, valid_dl, criterion, optimizer, scheduler, device)","metadata":{"execution":{"iopub.status.busy":"2024-08-03T07:14:32.066844Z","iopub.execute_input":"2024-08-03T07:14:32.067528Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"1/50: train_loss[3.6974] - val_loss[3.2037] - val_acc[0.2240]\n2/50: train_loss[2.9949] - val_loss[2.7024] - val_acc[0.3159]\n3/50: train_loss[2.5894] - val_loss[2.4189] - val_acc[0.3806]\n4/50: train_loss[2.3071] - val_loss[2.1875] - val_acc[0.4263]\n5/50: train_loss[2.0841] - val_loss[2.1099] - val_acc[0.4393]\n6/50: train_loss[1.9077] - val_loss[1.9442] - val_acc[0.4764]\n7/50: train_loss[1.7824] - val_loss[1.8338] - val_acc[0.4978]\n8/50: train_loss[1.6692] - val_loss[1.7553] - val_acc[0.5236]\n9/50: train_loss[1.5732] - val_loss[1.7183] - val_acc[0.5256]\n10/50: train_loss[1.5006] - val_loss[1.6318] - val_acc[0.5493]\n11/50: train_loss[1.4165] - val_loss[1.6224] - val_acc[0.5557]\n12/50: train_loss[1.3605] - val_loss[1.5409] - val_acc[0.5787]\n13/50: train_loss[1.3045] - val_loss[1.5108] - val_acc[0.5831]\n14/50: train_loss[1.2435] - val_loss[1.5175] - val_acc[0.5849]\n15/50: train_loss[1.1947] - val_loss[1.4550] - val_acc[0.6044]\n16/50: train_loss[1.1469] - val_loss[1.4488] - val_acc[0.6062]\n17/50: train_loss[1.1121] - val_loss[1.4654] - val_acc[0.6012]\n18/50: train_loss[1.0723] - val_loss[1.4096] - val_acc[0.6138]\n19/50: train_loss[1.0332] - val_loss[1.4474] - val_acc[0.6110]\n20/50: train_loss[0.9995] - val_loss[1.3951] - val_acc[0.6232]\n21/50: train_loss[0.9643] - val_loss[1.4126] - val_acc[0.6176]\n22/50: train_loss[0.9350] - val_loss[1.3716] - val_acc[0.6328]\n23/50: train_loss[0.9002] - val_loss[1.4258] - val_acc[0.6238]\n24/50: train_loss[0.8579] - val_loss[1.3533] - val_acc[0.6330]\n25/50: train_loss[0.8475] - val_loss[1.3629] - val_acc[0.6354]\n26/50: train_loss[0.8110] - val_loss[1.3491] - val_acc[0.6374]\n27/50: train_loss[0.7735] - val_loss[1.2905] - val_acc[0.6530]\n28/50: train_loss[0.7463] - val_loss[1.3131] - val_acc[0.6538]\n29/50: train_loss[0.7171] - val_loss[1.3363] - val_acc[0.6374]\n30/50: train_loss[0.6907] - val_loss[1.3642] - val_acc[0.6384]\n31/50: train_loss[0.6777] - val_loss[1.3107] - val_acc[0.6512]\n32/50: train_loss[0.6503] - val_loss[1.3476] - val_acc[0.6488]\n33/50: train_loss[0.6265] - val_loss[1.3122] - val_acc[0.6536]\n","output_type":"stream"}]},{"cell_type":"code","source":"train_history2, valid_history2 = fit_network(model, EPOCHS, train_dl, valid_dl, criterion, optimizer, scheduler, device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_training_history(train_history + train_history1 + train_history2, valid_history + valid_history1 + valid_history2)","metadata":{},"execution_count":null,"outputs":[]}]}