{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1h-Mf6qbR7tZqei97yyj9pXsemmGp3CgY",
      "authorship_tag": "ABX9TyPpmkoicP5eR6PdS/284ZpP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SCCSMARTCODE/Deep-Learning-00/blob/main/ASL_Alphabet/ASL_Alphabet_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iyruGI3hL7xp"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Extracting my zip file\n",
        "\"\"\"\n",
        "import zipfile\n",
        "\n",
        "PATH=\"/content/drive/MyDrive/Deep Learning/ASL Alphabet/archive.zip\"\n",
        "EXTRACTED_FILE_PATH=\".\"\n",
        "\n",
        "with zipfile.ZipFile(PATH, 'r') as f:\n",
        "    f.extractall(EXTRACTED_FILE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision.transforms as tt\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "\n",
        "TRAINING_DATA_PATH = \"asl_alphabet_train/asl_alphabet_train\"\n",
        "TESTING_DATA_PATH = \"asl_alphabet_test/\""
      ],
      "metadata": {
        "id": "T_zzdAYcxhn4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats = ([0.4666, 0.4576, 0.4699], [0.1974, 0.2367, 0.2523])\n",
        "\n",
        "train_transform = tt.Compose([\n",
        "    tt.Resize((128, 128)),\n",
        "    tt.ToTensor(),\n",
        "    tt.RandomHorizontalFlip(),\n",
        "    tt.RandomRotation(degrees=10),\n",
        "    tt.Normalize(*stats, inplace=False)\n",
        "])\n",
        "\n",
        "raw_train_dataset = ImageFolder(root=TRAINING_DATA_PATH, transform=train_transform)\n",
        "raw_test_dataset = ImageFolder(root=TESTING_DATA_PATH, transform=tt.ToTensor())"
      ],
      "metadata": {
        "id": "FH5BNPDgx0WG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(raw_train_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pucvpdiA_jM6",
        "outputId": "5b6a1fe3-1a58-4a75-c5c4-730710035f3f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, val_dataset = random_split(raw_train_dataset, [86000, 1000])\n",
        "\n",
        "train_dl = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_dl = DataLoader(val_dataset, batch_size=128, num_workers=2, pin_memory=True)\n",
        "test_dl = DataLoader(raw_test_dataset, batch_size=8)"
      ],
      "metadata": {
        "id": "VSym-zVyzhVn"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Designing The ASLNetwork\n",
        "\"\"\"\n",
        "\n",
        "class ASLNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ASLNetwork, self).__init__()\n",
        "\n",
        "        # Initial Convolutional Block\n",
        "        self.initial_conv_block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.BatchNorm2d(128)\n",
        "        )\n",
        "\n",
        "        # Residual Block 1\n",
        "        self.residual_block1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "        )\n",
        "\n",
        "        # Intermediate Convolutional Block\n",
        "        self.intermediate_conv_block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=128, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "\n",
        "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.BatchNorm2d(1024),\n",
        "\n",
        "            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.BatchNorm2d(1024)\n",
        "        )\n",
        "\n",
        "        # Residual Block 2\n",
        "        self.residual_block2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(1024),\n",
        "\n",
        "            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(1024),\n",
        "\n",
        "            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(1024),\n",
        "        )\n",
        "\n",
        "        # Final Convolutional Block\n",
        "        self.final_conv_block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1024, out_channels=2048, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.BatchNorm2d(2048),\n",
        "\n",
        "            nn.Conv2d(in_channels=2048, out_channels=2048, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.BatchNorm2d(2048),\n",
        "        )\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        self.fully_connected = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048 * 2 * 2, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.BatchNorm1d(512),\n",
        "\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.BatchNorm1d(256),\n",
        "\n",
        "            nn.Linear(256, 29),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.initial_conv_block(x)\n",
        "        x = x + self.residual_block1(x)\n",
        "        x = self.intermediate_conv_block(x)\n",
        "        x = x + self.residual_block2(x)\n",
        "        x = self.final_conv_block(x)\n",
        "        x = self.fully_connected(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate the model and move to GPU if available\n",
        "model = ASLNetwork()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "id": "7UNJOmLmJzHs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "875998a3-bdfa-4f2f-fd7a-719c397ffcff"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ASLNetwork(\n",
              "  (initial_conv_block): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): ReLU()\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU()\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (residual_block1): Sequential(\n",
              "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): ReLU()\n",
              "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU()\n",
              "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (intermediate_conv_block): Sequential(\n",
              "    (0): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): ReLU()\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU()\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (residual_block2): Sequential(\n",
              "    (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): ReLU()\n",
              "    (5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU()\n",
              "    (8): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (final_conv_block): Sequential(\n",
              "    (0): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (5): ReLU()\n",
              "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (7): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (fully_connected): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=8192, out_features=512, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (6): ReLU()\n",
              "    (7): Dropout(p=0.5, inplace=False)\n",
              "    (8): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (9): Linear(in_features=256, out_features=29, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE=1e-4\n",
        "MAX_LR=0.0001\n",
        "EPOCHS=1\n",
        "WEIGHT_DECAY = 1e-4\n",
        "BETA1 = 0.9\n",
        "BETA2 = 0.999"
      ],
      "metadata": {
        "id": "1HjfmK-2FagD"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(model.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2), weight_decay=WEIGHT_DECAY) #Issues with this work\n",
        "scheduler = OneCycleLR(optimizer, MAX_LR, epochs=EPOCHS, steps_per_epoch=len(train_dl))\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "scaler = GradScaler()"
      ],
      "metadata": {
        "id": "ahREPwb0BWSp"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(batch_loader, model):\n",
        "    model.eval()\n",
        "    accurate_pred = 0\n",
        "    pred_count = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in batch_loader:\n",
        "            inputs = inputs.to(device).half()\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            with autocast():\n",
        "                preds = model(inputs)\n",
        "                # print(nn.Softmax(1)(preds))\n",
        "                percent, predicted = torch.max(preds, 1)\n",
        "            accurate_pred += (predicted == labels).sum().item()\n",
        "            pred_count += labels.size(0)\n",
        "\n",
        "    return accurate_pred / pred_count\n",
        "\n",
        "\n",
        "def evaluate(batch_loader, model, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in batch_loader:\n",
        "            inputs = inputs.to(device).half()\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            with autocast():\n",
        "                preds = model(inputs)\n",
        "                loss = criterion(preds, labels)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(batch_loader)\n"
      ],
      "metadata": {
        "id": "ImMPmUq-Eslq"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(epochs, train_dl, val_dl, optimizer, criterion, scheduler, model):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "\n",
        "        for inputs, labels in train_dl:\n",
        "            inputs = inputs.to(device).half()\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast():\n",
        "                pred = model(inputs)\n",
        "                loss = criterion(pred, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= len(train_dl)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        val_loss = evaluate(val_dl, model, criterion)\n",
        "        val_accuracy = accuracy(val_dl, model)\n",
        "\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy * 100:.2f}%\")\n",
        "\n",
        "    return train_losses, val_losses, val_accuracies"
      ],
      "metadata": {
        "id": "jWQ-FHLPENJg"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, val_losses, val_accuracies = fit(EPOCHS, train_dl, val_dl, optimizer, criterion, scheduler, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrH9IIDfGN8j",
        "outputId": "f0545196-27f0-44db-c9de-48600d868992"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Train Loss: 0.0014, Val Loss: 0.0001, Val Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/Deep Learning/ASL Alphabet/parameter.pth\")"
      ],
      "metadata": {
        "id": "TrNwVeziNdYL"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, val_losses, val_accuracies = fit(EPOCHS, val_dl, val_dl, optimizer, criterion, scheduler, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1yhqXN870B3",
        "outputId": "40ededd0-5855-4fce-aa4e-df50da41c045"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Train Loss: 0.0016, Val Loss: 0.0002, Val Accuracy: 100.00%\n",
            "Epoch 2/3, Train Loss: 0.0019, Val Loss: 0.0002, Val Accuracy: 100.00%\n",
            "Epoch 3/3, Train Loss: 0.0014, Val Loss: 0.0002, Val Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(evaluate(val_dl, model, criterion=criterion))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zi9JauExFQem",
        "outputId": "4ecfefed-c761-4d10-e257-ca5535518b08"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8.941562646214152e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Yxs_x_irR_DP"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}